<!DOCTYPE html>
<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>

<title>The Design of quanteda</title>

<script type="text/javascript">
window.onload = function() {
  var imgs = document.getElementsByTagName('img'), i, img;
  for (i = 0; i < imgs.length; i++) {
    img = imgs[i];
    // center an image if it is the only element of its parent
    if (img.parentElement.childElementCount === 1)
      img.parentElement.style.textAlign = 'center';
  }
};
</script>

<!-- Styles for R syntax highlighter -->
<style type="text/css">
   pre .operator,
   pre .paren {
     color: rgb(104, 118, 135)
   }

   pre .literal {
     color: #990073
   }

   pre .number {
     color: #099;
   }

   pre .comment {
     color: #998;
     font-style: italic
   }

   pre .keyword {
     color: #900;
     font-weight: bold
   }

   pre .identifier {
     color: rgb(0, 0, 0);
   }

   pre .string {
     color: #d14;
   }
</style>

<!-- R syntax highlighter -->
<script type="text/javascript">
var hljs=new function(){function m(p){return p.replace(/&/gm,"&amp;").replace(/</gm,"&lt;")}function f(r,q,p){return RegExp(q,"m"+(r.cI?"i":"")+(p?"g":""))}function b(r){for(var p=0;p<r.childNodes.length;p++){var q=r.childNodes[p];if(q.nodeName=="CODE"){return q}if(!(q.nodeType==3&&q.nodeValue.match(/\s+/))){break}}}function h(t,s){var p="";for(var r=0;r<t.childNodes.length;r++){if(t.childNodes[r].nodeType==3){var q=t.childNodes[r].nodeValue;if(s){q=q.replace(/\n/g,"")}p+=q}else{if(t.childNodes[r].nodeName=="BR"){p+="\n"}else{p+=h(t.childNodes[r])}}}if(/MSIE [678]/.test(navigator.userAgent)){p=p.replace(/\r/g,"\n")}return p}function a(s){var r=s.className.split(/\s+/);r=r.concat(s.parentNode.className.split(/\s+/));for(var q=0;q<r.length;q++){var p=r[q].replace(/^language-/,"");if(e[p]){return p}}}function c(q){var p=[];(function(s,t){for(var r=0;r<s.childNodes.length;r++){if(s.childNodes[r].nodeType==3){t+=s.childNodes[r].nodeValue.length}else{if(s.childNodes[r].nodeName=="BR"){t+=1}else{if(s.childNodes[r].nodeType==1){p.push({event:"start",offset:t,node:s.childNodes[r]});t=arguments.callee(s.childNodes[r],t);p.push({event:"stop",offset:t,node:s.childNodes[r]})}}}}return t})(q,0);return p}function k(y,w,x){var q=0;var z="";var s=[];function u(){if(y.length&&w.length){if(y[0].offset!=w[0].offset){return(y[0].offset<w[0].offset)?y:w}else{return w[0].event=="start"?y:w}}else{return y.length?y:w}}function t(D){var A="<"+D.nodeName.toLowerCase();for(var B=0;B<D.attributes.length;B++){var C=D.attributes[B];A+=" "+C.nodeName.toLowerCase();if(C.value!==undefined&&C.value!==false&&C.value!==null){A+='="'+m(C.value)+'"'}}return A+">"}while(y.length||w.length){var v=u().splice(0,1)[0];z+=m(x.substr(q,v.offset-q));q=v.offset;if(v.event=="start"){z+=t(v.node);s.push(v.node)}else{if(v.event=="stop"){var p,r=s.length;do{r--;p=s[r];z+=("</"+p.nodeName.toLowerCase()+">")}while(p!=v.node);s.splice(r,1);while(r<s.length){z+=t(s[r]);r++}}}}return z+m(x.substr(q))}function j(){function q(x,y,v){if(x.compiled){return}var u;var s=[];if(x.k){x.lR=f(y,x.l||hljs.IR,true);for(var w in x.k){if(!x.k.hasOwnProperty(w)){continue}if(x.k[w] instanceof Object){u=x.k[w]}else{u=x.k;w="keyword"}for(var r in u){if(!u.hasOwnProperty(r)){continue}x.k[r]=[w,u[r]];s.push(r)}}}if(!v){if(x.bWK){x.b="\\b("+s.join("|")+")\\s"}x.bR=f(y,x.b?x.b:"\\B|\\b");if(!x.e&&!x.eW){x.e="\\B|\\b"}if(x.e){x.eR=f(y,x.e)}}if(x.i){x.iR=f(y,x.i)}if(x.r===undefined){x.r=1}if(!x.c){x.c=[]}x.compiled=true;for(var t=0;t<x.c.length;t++){if(x.c[t]=="self"){x.c[t]=x}q(x.c[t],y,false)}if(x.starts){q(x.starts,y,false)}}for(var p in e){if(!e.hasOwnProperty(p)){continue}q(e[p].dM,e[p],true)}}function d(B,C){if(!j.called){j();j.called=true}function q(r,M){for(var L=0;L<M.c.length;L++){if((M.c[L].bR.exec(r)||[null])[0]==r){return M.c[L]}}}function v(L,r){if(D[L].e&&D[L].eR.test(r)){return 1}if(D[L].eW){var M=v(L-1,r);return M?M+1:0}return 0}function w(r,L){return L.i&&L.iR.test(r)}function K(N,O){var M=[];for(var L=0;L<N.c.length;L++){M.push(N.c[L].b)}var r=D.length-1;do{if(D[r].e){M.push(D[r].e)}r--}while(D[r+1].eW);if(N.i){M.push(N.i)}return f(O,M.join("|"),true)}function p(M,L){var N=D[D.length-1];if(!N.t){N.t=K(N,E)}N.t.lastIndex=L;var r=N.t.exec(M);return r?[M.substr(L,r.index-L),r[0],false]:[M.substr(L),"",true]}function z(N,r){var L=E.cI?r[0].toLowerCase():r[0];var M=N.k[L];if(M&&M instanceof Array){return M}return false}function F(L,P){L=m(L);if(!P.k){return L}var r="";var O=0;P.lR.lastIndex=0;var M=P.lR.exec(L);while(M){r+=L.substr(O,M.index-O);var N=z(P,M);if(N){x+=N[1];r+='<span class="'+N[0]+'">'+M[0]+"</span>"}else{r+=M[0]}O=P.lR.lastIndex;M=P.lR.exec(L)}return r+L.substr(O,L.length-O)}function J(L,M){if(M.sL&&e[M.sL]){var r=d(M.sL,L);x+=r.keyword_count;return r.value}else{return F(L,M)}}function I(M,r){var L=M.cN?'<span class="'+M.cN+'">':"";if(M.rB){y+=L;M.buffer=""}else{if(M.eB){y+=m(r)+L;M.buffer=""}else{y+=L;M.buffer=r}}D.push(M);A+=M.r}function G(N,M,Q){var R=D[D.length-1];if(Q){y+=J(R.buffer+N,R);return false}var P=q(M,R);if(P){y+=J(R.buffer+N,R);I(P,M);return P.rB}var L=v(D.length-1,M);if(L){var O=R.cN?"</span>":"";if(R.rE){y+=J(R.buffer+N,R)+O}else{if(R.eE){y+=J(R.buffer+N,R)+O+m(M)}else{y+=J(R.buffer+N+M,R)+O}}while(L>1){O=D[D.length-2].cN?"</span>":"";y+=O;L--;D.length--}var r=D[D.length-1];D.length--;D[D.length-1].buffer="";if(r.starts){I(r.starts,"")}return R.rE}if(w(M,R)){throw"Illegal"}}var E=e[B];var D=[E.dM];var A=0;var x=0;var y="";try{var s,u=0;E.dM.buffer="";do{s=p(C,u);var t=G(s[0],s[1],s[2]);u+=s[0].length;if(!t){u+=s[1].length}}while(!s[2]);if(D.length>1){throw"Illegal"}return{r:A,keyword_count:x,value:y}}catch(H){if(H=="Illegal"){return{r:0,keyword_count:0,value:m(C)}}else{throw H}}}function g(t){var p={keyword_count:0,r:0,value:m(t)};var r=p;for(var q in e){if(!e.hasOwnProperty(q)){continue}var s=d(q,t);s.language=q;if(s.keyword_count+s.r>r.keyword_count+r.r){r=s}if(s.keyword_count+s.r>p.keyword_count+p.r){r=p;p=s}}if(r.language){p.second_best=r}return p}function i(r,q,p){if(q){r=r.replace(/^((<[^>]+>|\t)+)/gm,function(t,w,v,u){return w.replace(/\t/g,q)})}if(p){r=r.replace(/\n/g,"<br>")}return r}function n(t,w,r){var x=h(t,r);var v=a(t);var y,s;if(v){y=d(v,x)}else{return}var q=c(t);if(q.length){s=document.createElement("pre");s.innerHTML=y.value;y.value=k(q,c(s),x)}y.value=i(y.value,w,r);var u=t.className;if(!u.match("(\\s|^)(language-)?"+v+"(\\s|$)")){u=u?(u+" "+v):v}if(/MSIE [678]/.test(navigator.userAgent)&&t.tagName=="CODE"&&t.parentNode.tagName=="PRE"){s=t.parentNode;var p=document.createElement("div");p.innerHTML="<pre><code>"+y.value+"</code></pre>";t=p.firstChild.firstChild;p.firstChild.cN=s.cN;s.parentNode.replaceChild(p.firstChild,s)}else{t.innerHTML=y.value}t.className=u;t.result={language:v,kw:y.keyword_count,re:y.r};if(y.second_best){t.second_best={language:y.second_best.language,kw:y.second_best.keyword_count,re:y.second_best.r}}}function o(){if(o.called){return}o.called=true;var r=document.getElementsByTagName("pre");for(var p=0;p<r.length;p++){var q=b(r[p]);if(q){n(q,hljs.tabReplace)}}}function l(){if(window.addEventListener){window.addEventListener("DOMContentLoaded",o,false);window.addEventListener("load",o,false)}else{if(window.attachEvent){window.attachEvent("onload",o)}else{window.onload=o}}}var e={};this.LANGUAGES=e;this.highlight=d;this.highlightAuto=g;this.fixMarkup=i;this.highlightBlock=n;this.initHighlighting=o;this.initHighlightingOnLoad=l;this.IR="[a-zA-Z][a-zA-Z0-9_]*";this.UIR="[a-zA-Z_][a-zA-Z0-9_]*";this.NR="\\b\\d+(\\.\\d+)?";this.CNR="\\b(0[xX][a-fA-F0-9]+|(\\d+(\\.\\d*)?|\\.\\d+)([eE][-+]?\\d+)?)";this.BNR="\\b(0b[01]+)";this.RSR="!|!=|!==|%|%=|&|&&|&=|\\*|\\*=|\\+|\\+=|,|\\.|-|-=|/|/=|:|;|<|<<|<<=|<=|=|==|===|>|>=|>>|>>=|>>>|>>>=|\\?|\\[|\\{|\\(|\\^|\\^=|\\||\\|=|\\|\\||~";this.ER="(?![\\s\\S])";this.BE={b:"\\\\.",r:0};this.ASM={cN:"string",b:"'",e:"'",i:"\\n",c:[this.BE],r:0};this.QSM={cN:"string",b:'"',e:'"',i:"\\n",c:[this.BE],r:0};this.CLCM={cN:"comment",b:"//",e:"$"};this.CBLCLM={cN:"comment",b:"/\\*",e:"\\*/"};this.HCM={cN:"comment",b:"#",e:"$"};this.NM={cN:"number",b:this.NR,r:0};this.CNM={cN:"number",b:this.CNR,r:0};this.BNM={cN:"number",b:this.BNR,r:0};this.inherit=function(r,s){var p={};for(var q in r){p[q]=r[q]}if(s){for(var q in s){p[q]=s[q]}}return p}}();hljs.LANGUAGES.cpp=function(){var a={keyword:{"false":1,"int":1,"float":1,"while":1,"private":1,"char":1,"catch":1,"export":1,virtual:1,operator:2,sizeof:2,dynamic_cast:2,typedef:2,const_cast:2,"const":1,struct:1,"for":1,static_cast:2,union:1,namespace:1,unsigned:1,"long":1,"throw":1,"volatile":2,"static":1,"protected":1,bool:1,template:1,mutable:1,"if":1,"public":1,friend:2,"do":1,"return":1,"goto":1,auto:1,"void":2,"enum":1,"else":1,"break":1,"new":1,extern:1,using:1,"true":1,"class":1,asm:1,"case":1,typeid:1,"short":1,reinterpret_cast:2,"default":1,"double":1,register:1,explicit:1,signed:1,typename:1,"try":1,"this":1,"switch":1,"continue":1,wchar_t:1,inline:1,"delete":1,alignof:1,char16_t:1,char32_t:1,constexpr:1,decltype:1,noexcept:1,nullptr:1,static_assert:1,thread_local:1,restrict:1,_Bool:1,complex:1},built_in:{std:1,string:1,cin:1,cout:1,cerr:1,clog:1,stringstream:1,istringstream:1,ostringstream:1,auto_ptr:1,deque:1,list:1,queue:1,stack:1,vector:1,map:1,set:1,bitset:1,multiset:1,multimap:1,unordered_set:1,unordered_map:1,unordered_multiset:1,unordered_multimap:1,array:1,shared_ptr:1}};return{dM:{k:a,i:"</",c:[hljs.CLCM,hljs.CBLCLM,hljs.QSM,{cN:"string",b:"'\\\\?.",e:"'",i:"."},{cN:"number",b:"\\b(\\d+(\\.\\d*)?|\\.\\d+)(u|U|l|L|ul|UL|f|F)"},hljs.CNM,{cN:"preprocessor",b:"#",e:"$"},{cN:"stl_container",b:"\\b(deque|list|queue|stack|vector|map|set|bitset|multiset|multimap|unordered_map|unordered_set|unordered_multiset|unordered_multimap|array)\\s*<",e:">",k:a,r:10,c:["self"]}]}}}();hljs.LANGUAGES.r={dM:{c:[hljs.HCM,{cN:"number",b:"\\b0[xX][0-9a-fA-F]+[Li]?\\b",e:hljs.IMMEDIATE_RE,r:0},{cN:"number",b:"\\b\\d+(?:[eE][+\\-]?\\d*)?L\\b",e:hljs.IMMEDIATE_RE,r:0},{cN:"number",b:"\\b\\d+\\.(?!\\d)(?:i\\b)?",e:hljs.IMMEDIATE_RE,r:1},{cN:"number",b:"\\b\\d+(?:\\.\\d*)?(?:[eE][+\\-]?\\d*)?i?\\b",e:hljs.IMMEDIATE_RE,r:0},{cN:"number",b:"\\.\\d+(?:[eE][+\\-]?\\d*)?i?\\b",e:hljs.IMMEDIATE_RE,r:1},{cN:"keyword",b:"(?:tryCatch|library|setGeneric|setGroupGeneric)\\b",e:hljs.IMMEDIATE_RE,r:10},{cN:"keyword",b:"\\.\\.\\.",e:hljs.IMMEDIATE_RE,r:10},{cN:"keyword",b:"\\.\\.\\d+(?![\\w.])",e:hljs.IMMEDIATE_RE,r:10},{cN:"keyword",b:"\\b(?:function)",e:hljs.IMMEDIATE_RE,r:2},{cN:"keyword",b:"(?:if|in|break|next|repeat|else|for|return|switch|while|try|stop|warning|require|attach|detach|source|setMethod|setClass)\\b",e:hljs.IMMEDIATE_RE,r:1},{cN:"literal",b:"(?:NA|NA_integer_|NA_real_|NA_character_|NA_complex_)\\b",e:hljs.IMMEDIATE_RE,r:10},{cN:"literal",b:"(?:NULL|TRUE|FALSE|T|F|Inf|NaN)\\b",e:hljs.IMMEDIATE_RE,r:1},{cN:"identifier",b:"[a-zA-Z.][a-zA-Z0-9._]*\\b",e:hljs.IMMEDIATE_RE,r:0},{cN:"operator",b:"<\\-(?!\\s*\\d)",e:hljs.IMMEDIATE_RE,r:2},{cN:"operator",b:"\\->|<\\-",e:hljs.IMMEDIATE_RE,r:1},{cN:"operator",b:"%%|~",e:hljs.IMMEDIATE_RE},{cN:"operator",b:">=|<=|==|!=|\\|\\||&&|=|\\+|\\-|\\*|/|\\^|>|<|!|&|\\||\\$|:",e:hljs.IMMEDIATE_RE,r:0},{cN:"operator",b:"%",e:"%",i:"\\n",r:1},{cN:"identifier",b:"`",e:"`",r:0},{cN:"string",b:'"',e:'"',c:[hljs.BE],r:0},{cN:"string",b:"'",e:"'",c:[hljs.BE],r:0},{cN:"paren",b:"[[({\\])}]",e:hljs.IMMEDIATE_RE,r:0}]}};
hljs.initHighlightingOnLoad();
</script>



<style type="text/css">
body, td {
   font-family: sans-serif;
   background-color: white;
   font-size: 13px;
}

body {
  max-width: 800px;
  margin: auto;
  padding: 1em;
  line-height: 20px;
}

tt, code, pre {
   font-family: 'DejaVu Sans Mono', 'Droid Sans Mono', 'Lucida Console', Consolas, Monaco, monospace;
}

h1 {
   font-size:2.2em;
}

h2 {
   font-size:1.8em;
}

h3 {
   font-size:1.4em;
}

h4 {
   font-size:1.0em;
}

h5 {
   font-size:0.9em;
}

h6 {
   font-size:0.8em;
}

a:visited {
   color: rgb(50%, 0%, 50%);
}

pre, img {
  max-width: 100%;
}
pre {
  overflow-x: auto;
}
pre code {
   display: block; padding: 0.5em;
}

code {
  font-size: 92%;
  border: 1px solid #ccc;
}

code[class] {
  background-color: #F8F8F8;
}

table, td, th {
  border: none;
}

blockquote {
   color:#666666;
   margin:0;
   padding-left: 1em;
   border-left: 0.5em #EEE solid;
}

hr {
   height: 0px;
   border-bottom: none;
   border-top-width: thin;
   border-top-style: dotted;
   border-top-color: #999999;
}

@media print {
   * {
      background: transparent !important;
      color: black !important;
      filter:none !important;
      -ms-filter: none !important;
   }

   body {
      font-size:12pt;
      max-width:100%;
   }

   a, a:visited {
      text-decoration: underline;
   }

   hr {
      visibility: hidden;
      page-break-before: always;
   }

   pre, blockquote {
      padding-right: 1em;
      page-break-inside: avoid;
   }

   tr, img {
      page-break-inside: avoid;
   }

   img {
      max-width: 100% !important;
   }

   @page :left {
      margin: 15mm 20mm 15mm 10mm;
   }

   @page :right {
      margin: 15mm 10mm 15mm 20mm;
   }

   p, h2, h3 {
      orphans: 3; widows: 3;
   }

   h2, h3 {
      page-break-after: avoid;
   }
}
</style>



</head>

<body>
<h1>The Design of quanteda</h1>

<h2>Basic Principles</h2>

<ol>
<li><p><strong>Corpus texts should remain <em>unchanged</em> during subsequent analysis and processing.</strong>  In other words, after <em>loading</em> and <em>encoding</em>, we should discourage users from modifying a corpus of texts as a form of processing, so that the corpus can act as a library and record of the original texts, prior to any downstream processing.  This not only aids in replication, but also means that a corpus presents the unmodified texts to which any processing, feature selection, transformations, or sampling may be applied or reapplied, without hard-coding any changes made as part of the process of analyzing the texts.  The only exception is to reshape the units of text in a corpus, but we will record the details of this reshaping to make it relatively easy to reverse unit changes.  Since the definition of a &ldquo;document&rdquo; is part of the process of loading texts into a corpus, however, rather than processing, we will take a less stringent line on this aspect of changing a corpus.</p></li>
<li><p><strong>A corpus should be capable of holding additional objects that will be associated with the corpus, such as dictionaries, stopword, and phrase lists.</strong>  These will be named objects, that can be invoked when using (for instance) <code>dfm()</code>.  This allows a corpus to contain all of the additional objects that would normally be associated with it, rather than requiring a set of separate, extra-corpus objects.</p></li>
<li><p><strong>A tokenized text object, and a dfm object, should have settings that record the processing options applied to the texts or corpus from which they were created.</strong>  These provide a record of what was done to the text, and where it came from.  Examples are <code>tolower</code>, <code>stem</code>, <code>removeTwitter</code>, etc.  They also include any objects used in feature selection, such as dictionaries or stopword lists.</p></li>
<li><p><strong>A dfm should consist mainly of a (sparse) matrix,</strong> that can be used for any sort of quantitative analysis.  The basic structure will always be <em>documents</em> (or document groups) in rows by <em>features</em> in columns.</p></li>
<li><p><strong>Encoding of texts</strong> should be done in the corpus, and recorded as meta-data in the corpus.  We should be able to detect encodings and suggest (and perform) and conversion when storing texts in a corpus.  This encoding should be <code>UTF-8</code> by default.  We will use the tools available in the <code>stringi</code> package to detect and set character encodings, namely <code>stri_enc_detect()</code> and <code>stri_conv()</code>, with reports and suggestions made at the time of corpus creation.</p></li>
</ol>

<h2>Major categories of functions</h2>

<ol>
<li><p><strong>Corpus construction and management</strong>.  These operate on a corpus, and return a corpus, or report on a corpus.</p>

<pre><code class="r">changeunits
corpus
docnames, &lt;-
docvars, &lt;-
encoding, &lt;-
language, &lt;-
metacorpus, metadoc, &lt;-
ndoc
ntoken
segment      # also works on character vectors
settings, &lt;-
subset
summary
textfile
texts
</code></pre></li>
<li><p><strong>Text manipulation.</strong>  These operate on character vectors, and return character vectors.</p>

<p><strong>NOTE: This is a bit out of date now &ndash; we are working on a table by function of input and output objects.</strong></p>

<ol>
<li><p>Operations on character vectors or the character vector of texts from a corpus.</p>

<p>Returns a list of character vectors:</p>

<pre><code class="r">tokenize
</code></pre>

<p>Returning a character vector:</p>

<pre><code class="r">phrasetotoken
</code></pre>

<p>Returns a <code>collocations</code> object, </p>

<pre><code class="r">collocations
</code></pre>

<p>Returns a screen output and data/frame: </p>

<pre><code class="r">kwic
</code></pre>

<p>Counts the number of tokens:</p>

<pre><code class="r">ntoken
</code></pre></li>
<li><p>Operations on character vectors of <em>tokens only</em>, returning a character vector of tokens:</p>

<pre><code class="r">syllables
wordstem
</code></pre></li>
<li><p>Operations on character vectors of <em>tokens</em>, but also dfm objects and collocations:</p>

<pre><code class="r">removeFeatures
</code></pre></li>
<li><p>Operations that <em>previously</em> worked (currently work) on character vectors of any size, but that will now be folded into Workflow Step 2 functions (see below) as part of tokenize:</p>

<pre><code class="r">ngrams
skipgrams
</code></pre></li>
</ol></li>
<li><p><strong><code>dfm</code> construction and manipulation.</strong></p>

<pre><code class="r">dfm         # also works directly on (the texts of) a corpus
convert
docfreq
docnames
features
lexdiv
ndoc
ntoken
plot
print, show
removeFeatures
similarity
sort
textmodel, textmodel_*
topfeatures
trim
weight
settings
</code></pre></li>
<li><p><strong>Auxiliary functions</strong>.</p>

<pre><code class="r">dictionary
stopwords
textfile
</code></pre></li>
<li><p><strong>Example datasets and objects</strong>.</p>

<p>Example data objects:</p>

<pre><code class="r">exampleString       # character, length 1
ukimmigTexts        # character, length 14
inaugTexts          # character, length 57
ie2010Corpus        # corpus
inaugCorpus         # corpus
LBGexample          # dfm
</code></pre>

<p>and some built-in objects used by functions:</p>

<pre><code class="r">englishSyllables    # named character vector, length 133245
stopwords           # named list .stopwords, length 16
</code></pre></li>
</ol>

<h1>Basic text analysis workflow</h1>

<h2>Working with a corpus, documents, and features</h2>

<ol>
<li><p><strong>Creating the corpus</strong></p>

<p>Reading files, probably using <code>textfile()</code>, then creating a corpus using <code>corpus()</code>, making sure the texts have a common encoding, and adding document variables (<code>docvars</code>) and metadata (<code>metadoc</code> and <code>metacorpus</code>).</p></li>
<li><p><strong>Defining and delimiting documents</strong></p>

<p>Defining what are &ldquo;texts&rdquo;, for instance using <code>changeunits</code> or grouping.  </p>

<p>Suggestion: add a <code>groups=</code> option to <code>texts()</code>, to extract texts from a corpus concatenated by groups of document variables.  (This functionality is currently only available through <code>dfm</code>.)</p></li>
<li><p><strong>Defining and delimiting textual features</strong></p>

<p>This step involves defining and extracting the relevant features from each document, using
<code>tokenize</code>, the main function for this step, involves indentifying instances of defined features (&ldquo;tokens&rdquo;) and extracting them as vectors.  Usually these will consist of words, but may also consist of:</p>

<ul>
<li>  <code>bigrams</code> or <code>ngrams</code>, adjacent sequences of words, not separated by punctuation marks or sentence boundaries; including</li>
<li>  multi-word expressions, through <code>phrasetotoken</code>, for selected word ngrams as identified in selected lists rather than simply using all adjacent word pairs or n-sequences.</li>
</ul>

<p><code>tokenize</code> returns a new object class of tokenized texts, which are essentially a list of character vectors, with each element in the list corresponding to a document, and each characte vector consisting of the tokens in that document. </p>

<p>By defining the broad class of tokens we wish to extract, in this step we also apply rules that will keep or ignore elements such as punctuation or digits, or special aggregations of word and other characters that make up URLS, Twitter tags, or currency-prefixed digits.  This will involve adding the following options to <code>tokenize</code>:</p>

<ul>
<li>  <code>removeDigits</code></li>
<li>  <code>removePunct</code></li>
<li>  <code>removeAdditional</code></li>
<li>  <code>removeTwitter</code></li>
<li>  <code>removeURL</code></li>
</ul>

<p><strong>By default</strong>, <code>tokenize()</code> extracts word tokens, and only <code>removeSeparators</code> is <code>TRUE</code>, meaning that <code>tokenize()</code> will return a list including punctuation as tokens.  This follows a philosophy of minimal intervention, and one requiring that additional decisions be made explicit by the user when invoking <code>tokenize()</code>.  Note that in the <code>dfm()</code> method described below, however, we do turn on all of these options except <code>removeTwitter</code>, which is by default <code>FALSE</code>.</p>

<p>For converting to lowercase, it is actually <em>faster</em> to perform this step <em>before</em> tokenization, but logically it falls under the next workflow step.  However for efficiency, <code>toLower()</code> works on </p>

<ul>
<li>  a corpus, returning a lower-cased character vector</li>
<li>  a character vector</li>
<li>  a list object of tokenized texts</li>
</ul>

<p>Since the tokenizer we will use may not distinguish the puncutation characters used in constructs such as URLs, email addresses, Twitter handles, or digits prefixed by currency symbols, we will mostly need to use a substitution strategy to replace these with alternative characters prior to tokenization, and then replace the substitutions with the original characters.  This will slow down processing but will only be active by explicit user request for this type of handling to take place.  We could offer three possible options here, such as for URLs, consisting of <code>c(&quot;ignore&quot;, &quot;keep&quot;, &quot;remove&quot;, &quot;ignore&quot;)</code>, to pretend they do not exist and tokenize come what may, to preserve remove URLs in their entirety as &ldquo;tokens&rdquo;, or to remove them completely, respectively.</p>

<p>Note that that defining and delimiting features may alao include their <em>parts of speech</em>, meaning we will need to add functionality for POS tagging and extraction in this step.</p></li>
<li><p><strong>Further feature selection</strong></p>

<p>Once features have been identified and separated from the texts in the tokenization step, features may be removed from token lists, or 
handled as part of <code>dfm</code> construction.  Features may be:</p>

<ul>
<li>  <em>eliminated</em> through use of predefined lists or patterns of <em>stop words</em>, using <code>removeFeatures</code> or <code>ignoredFeatures</code> (<code>dfm</code> option)</li>
<li>  <em>kept</em> through through use of predefined lists or patterns of <em>stop words</em>, using <code>removeFeatures</code> or <code>keptFeatures</code> (<code>dfm</code> option)</li>
<li>  <em>collapsed</em> by:

<ul>
<li>  considering morphological variations as equivalent to a stem or lemma, through the <code>stem</code> option in <code>dfm</code></li>
<li>  considering lists of features as equivalent to a <em>dictionary</em> key, either exclusively (<code>dfm</code> option <code>dictionary</code>) or as a supplement to uncollapsed features (<code>dfm</code> option <code>thesaurus</code>)</li>
<li>  <code>toLower</code> to consider as equivalent the same word features despite having different cases, by converting all features to lower case</li>
</ul></li>
</ul>

<p>It will be sometimes possible to perform these steps separately from the <code>dfm</code> creating stage, but in most cases these steps will be performed as options to the <code>dfm</code> function.</p></li>
<li><p><strong>Analysis of the documents and features</strong></p>

<ol>
<li><p>From a corpus.  </p>

<p>These steps don&#39;t necessarily require the processing steps above.</p>

<ul>
<li><code>kwic</code></li>
<li><code>lexdiv</code></li>
<li><code>summary</code></li>
</ul></li>
<li><p>From a dfm &ndash; after <code>dfm</code> on the processed document and features.</p></li>
</ol></li>
</ol>

<h2><code>dfm</code>, the Swiss Army knife</h2>

<h3>Overview</h3>

<ol>
<li><p>Most common use case</p>

<p>In most cases, users will use the default settings to create a dfm straight from a corpus.  <code>dfm</code> will combine steps 3&ndash;4, even though basic functions will be available to perform these separately.  All options shown in steps 3&ndash;4 will be available in <code>dfm</code>.</p></li>
<li><p>If separate steps are desired</p>

<p>We will do our best to ensure that all functions allow piping using the <code>magrittr</code> package, e.g.</p>

<pre><code class="r">mydfm &lt;- texts(mycorpus, group = &quot;party&quot;) %&gt;% toLower %&gt;% tokenize %&gt;% wordstem %&gt;%
                                removeFeatures(stopwords(&quot;english&quot;)) %&gt;% dfm
</code></pre>

<p>We recognize however that not all sequences will make sense, for instance <code>wordstem</code> will only work <em>after</em> tokenization, and will try to catch these errors and make the proper sequence clear to users.</p></li>
</ol>

<h3>Options for processing from corpus to dfm</h3>

<p>The current processing options, their defaults, and the function their value is finally passed to,  in in order of increasing generality:</p>

<hr/>

<p>Option          default     other                          function</p>

<hr/>

<p>keepAcronyms     FALSE       TRUE                           toLower</p>

<p>what             word        sentence, character            tokenize
                             fastestword, fasterword</p>

<p>cleanFirst       TRUE        FALSE                          tokenize</p>

<p>verbose          FALSE       TRUE                           tokenize, dfm</p>

<p>toLower          TRUE        FALSE                          tokenize, dfm</p>

<p>removeNumbers    TRUE        FALSE                          tokenize, dfm</p>

<p>removePunct      TRUE        FALSE                          tokenize, dfm</p>

<p>removeSeparators TRUE        FALSE                          tokenize, dfm</p>

<p>removeTwitter    TRUE        FALSE                          tokenize</p>

<p>simplify         FALSE       TRUE                           tokenize</p>

<p>cores            detect      numeric                        tokenize</p>

<p>stem             FALSE       TRUE                           dfm</p>

<p>ignoredFeatures  NULL        stopwords(), character         dfm</p>

<p>keptFeatures     NULL        regex                          dfm</p>

<p>matrixType       sparse      dense                          dfm</p>

<p>language         english     character                      dfm</p>

<p>fromCorpus       FALSE       TRUE                           dfm</p>

<p>bigrams          FALSE       TRUE                           dfm</p>

<p>include.unigrams TRUE        FALSE                          dfm</p>

<p>thesaurus        NULL        list                           dfm</p>

<p>dictionary       NULL        list                           dfm</p>

<p>dictionary_regex FALSE       re                             dfm</p>

<p>addto            NULL        dfm                            dfm</p>

<hr/>

<h4>dfm creation with ie2010 Corpus</h4>

<p>A <code>dfm</code> object can be created using piping, or in one step:</p>

<pre><code class="r">mydfm &lt;- texts(ie2010Corpus, groups = &quot;party&quot;) %&gt;% toLower %&gt;% tokenize %&gt;% 
             removeFeatures(stopwords(&quot;english&quot;)) %&gt;% wordstem %&gt;% dfm

# same as:
mydfm2 &lt;- dfm(ie2010Corpus, groups = &quot;party&quot;, ignoredFeatures = stopwords(&quot;english&quot;), stem = TRUE)
</code></pre>

<h1>Development Guidance</h1>

<h2>Suggestions for using quanteda during development</h2>

<p><code>quanteda</code> is in development and will remain so until we declare a 1.0 version, at which time we will only add new functions, not change the names of 
existing ones.  In the meantime, we suggest:</p>

<ul>
<li>  use named formals in the function calls, rather than relying on the current ordering of formals,
for instance, use <code>tokenize(mytexts, what = &quot;sentence&quot;)</code> instead of <code>tokenize(mytexts, &quot;sentence&quot;)</code> &ndash; since the order is not stable; and also using named formals rather than relying on current defaults, e.g. <code>tokenize(mytexts, removePunct = FALSE)</code> since the default values are not stable.</li>
<li>  hope that we get to 1.0 quickly;</li>
<li>  help that process by sending us feedback stating what you think of the syntax, formal names, etc. from a user&#39;s perspective.</li>
</ul>

<h2>Notes to the <strong>quanteda</strong> team</h2>

<ol>
<li><p>All <strong>testing</strong> should be in tests/testthat/test_<name>.R.  No more haphazard tests in other locations.</p></li>
<li><p>For <strong>performance comprisons</strong>, we write up the results and document them in the vignette <code>performance_comparisons.Rmd.</code></p></li>
<li><p>Development and branches:  We add new features through <code>workingDev</code>.  Before merging this with <code>dev</code>, we make sure the build passes a full CRAN check.</p></li>
</ol>

<h2>For bug reports and feature requests</h2>

<p>Please use the issue page on the GitHub repository, or contact <a href="mailto:kbenoit@lse.ac.uk">kbenoit@lse.ac.uk</a> directly.</p>

<p>We <em>always</em> welcome hearing about your experiences (and problems!) in using quanteda, as additional use cases and problems you may encounter help us to make the package more functional and robust.</p>

<h1>Outstanding Tasks and Priorities</h1>

<h2>Completed</h2>

<ul>
<li>  <strong>DONE</strong> rewrite <code>encoding()</code> to detect encoding, and replace <code>iconv()</code> calls with <code>stringi::stri_encode()</code> in <code>corpus()</code></li>
<li>  rewrite to make use of <strong>stringi</strong> (and the new <code>tokenize()</code> based on that package):

<ul>
<li>  <strong>DONE</strong> <code>ntoken()</code><br/></li>
<li>  <strong>DONE</strong> <code>ntype()</code> and <code>nfeature()</code></li>
<li>  <strong>DONE</strong> <code>phrasetotoken()</code> </li>
</ul></li>
<li>  <strong>DONE</strong> encode <code>ie2010Corpus</code> (and see if CRAN lets us get away with it)</li>
<li>  <strong>DONE</strong> removed <code>language()</code></li>
<li>  add methods for <code>tokenizedTexts</code> objects:

<ul>
<li>  <strong>DONE</strong> <code>dfm.tokenizedTexts</code></li>
<li>  <strong>DONE</strong> <code>removeFeatures.tokenizedTexts</code></li>
<li>  <strong>DONE</strong> <code>syllables.tokenizedTexts</code></li>
</ul></li>
<li>  <strong>DONE</strong> <code>removeFeatures</code> now much faster, based on fixed binary matches and <code>stringi</code> character classes</li>
<li>  <strong>DONE</strong> added readability statistics through <code>readability()</code></li>
<li>  <strong>DONE</strong> added <code>nsentence()</code></li>
<li>  <strong>DONE</strong> <code>ngrams</code> added as an option to <code>tokenize()</code></li>
<li>  <strong>DONE</strong> <code>dfm</code> objects need a subset method for selection rows, not just columns</li>
<li>  <strong>DONE</strong> rewrite <code>segment()</code> to make use of new tokenizer that segments on sentences</li>
<li>  <strong>DONE</strong> make sure <code>corpus.VCorpus()</code> is fully working</li>
<li>  <strong>DONE</strong> rewrite <code>kwic</code> to use new tokenizer, and to allow searches for multi-word regular expressions</li>
</ul>

<h2>To Do Remaining</h2>

<ul>
<li>  fix <code>collocations()</code> behaviour, changed since new <code>tokenize()</code> (<strong>IN PROGRESS</strong>)</li>
<li>  <code>weight()</code> breaks for dfms with all zero count documents, for instance those that have been <code>trim()</code>med</li>
<li>  rewrite <code>lexdiv()</code> to make the API similar to <code>readability()</code> and to use data.table</li>
<li>  integrate <code>collocations</code> code for bigrams and trigrams and reduce the internal memory usage</li>
<li>  <code>dfm</code> documentation needs to group arguments into sections and describe how these correspond to the logical workflow</li>
<li>  need to figure out how to exclude specific signatures (especially S4 signatures) from the man (.Rd) pages.  For instance ?&ldquo;dfm-class&rdquo; has far more details on methods signatures than any user will find useful.</li>
<li>  Devise a scheme for <code>settings()</code> and figure out how to add additional objects to a corpus, namely one or more:

<ul>
<li>  dictionary objects</li>
<li>  collocation objects</li>
<li>  stopword lists</li>
</ul></li>
<li>  optimize <code>similarity()</code></li>
<li>  consider adopting ISO language names for functions such as <code>wordstem()</code>, <code>stopwords()</code>, and <code>syllables()</code></li>
<li>  <code>textmodel</code>: Devise and document a consistent, logical, and easy-to-use and remember scheme for textmodels.</li>
<li>  Move more functions to S4</li>
<li>  Documentation for <code>convert()</code> needs substantial work</li>
<li>  Vignettes:

<ul>
<li>  Add a Performance analysis and comparison vignette</li>
<li>  Replication vignette for Jockers, <em>Text Analysis with R for Students of Literature</em></li>
</ul></li>
<li>  stripplot for keywords occurrence across documents, serially</li>
<li>  Define full set of operators for dfmSparse and dfmDense.  Right now, only <code>+</code> is defined.</li>
<li>  <code>resample</code> functionality to enable resampling from different text units</li>
<li>  <code>index</code> (?) for pre-tokenizing and indexing a corpus</li>
<li>   specific textmodel types:

<ul>
<li>  Improve wordfish and round out methods</li>
<li>  Add CA scaling</li>
</ul></li>
<li>  incorporate a word2vec-type model (see <a href="https://github.com/bmschmidt/wordVectors">wordvectors</a>)</li>
<li>  add POS tagging, through an interface to the Stanford NLP library, or through an existing R package that provides this functionality</li>
</ul>

</body>

</html>
