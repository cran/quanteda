---
title: "quanteda Development Plans"
author: "Ken Benoit and Paul Nulty"
date: "2015-07-09"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Development Plans}
  %\VignetteEngine{knitr::rmarkdown}
  \usepackage[utf8]{inputenc}
---

## Suggestions for using quanteda during development

`quanteda` is in development and will remain so until we declare a 1.0 version, at which time we will only add new functions, not change the names of 
existing ones.  In the meantime, we suggest:

-   use named formals in the function calls, rather than relying on the current ordering of formals,
    for instance, use `tokenize(mytexts, what = "sentence")` instead of `tokenize(mytexts, "sentence")` -- since the order is not stable; and also using named formals rather than relying on current defaults, e.g. `tokenize(mytexts, removePunct = FALSE)` since the default values are not stable.
-   hope that we get to 1.0 quickly;
-   help that process by sending us feedback stating what you think of the syntax, formal names, etc. from a user's perspective.

## Notes to the **quanteda** team

1.  All **testing** should be in tests/testthat/test_<name>.R.  No more haphazard tests in other locations.

2.  For **performance comprisons**, we write up the results and document them in the vignette `performance_comparisons.Rmd.`

3.  Development and branches:  We add new features through `workingDev`.  Before merging this with `dev`, we make sure the build passes a full CRAN check.

## Completed

*   **DONE** rewrite `encoding()` to detect encoding, and replace `iconv()` calls with `stringi::stri_encode()` in `corpus()`
*   rewrite to make use of **stringi** (and the new `tokenize()` based on that package):
    *   **DONE** `ntoken()`  
    *   **DONE** `ntype()` and `nfeature()`
    *   **DONE** `phrasetotoken()` 
*   **DONE** encode `ie2010Corpus` (and see if CRAN lets us get away with it)
*   **DONE** removed `language()`
*   add methods for `tokenizedTexts` objects:
    *   **DONE** `dfm.tokenizedTexts`
    *   **DONE** `removeFeatures.tokenizedTexts`
    *   **DONE** `syllables.tokenizedTexts`
*   **DONE** `removeFeatures` now much faster, based on fixed binary matches and `stringi` character classes
*   **DONE** added readability statistics through `readability()`
*   **DONE** added `nsentence()`
*   **DONE** `ngrams` added as an option to `tokenize()`


## To Do Remaining

*   rewrite `lexdiv()` to make the API similar to `readability()` and to use data.table
*   rewrite `segment()` to make use of new tokenizer that segments on sentences
*   make `bigrams`, `ngrams` punctuation sensitive in the same way that `collocations` is currently
*   integrate `collocations` code for bigrams and trigrams and reduce the internal memory usage
*   make sure `corpus.VCorpus()` is fully working
*   `dfm` documentation needs to group arguments into sections and describe how these correspond to the logical workflow
*   need to figure out how to exclude specific signatures (especially S4 signatures) from the man (.Rd) pages.  For instance `?"dfm-class" has far more details on methods signatures than any user will find useful.
*   rewrite `kwic` to use new tokenizer, and to allow searches for multi-word regular expressions
*   Devise a scheme for `settings()` and figure out how to add additional objects to a corpus, namely one or more:
    *   dictionary objects
    *   collocation objects
    *   stopword lists
*   optimize `similarity()`
*   consider adopting ISO language names for functions such as `wordstem()`, `stopwords()`, and `syllables()`
*   `textmodel`: Devise and document a consistent, logical, and easy-to-use and remember scheme for textmodels.
*   Move more functions to S4
*   Documentation for `convert()` needs substantial work
*   Vignettes:
    *   Vastly revise the Workflow vignette
    *   Add a Performance analysis and comparison vignette
*   Define full set of operators for dfmSparse and dfmDense.  Right now, only `+` is defined.
*   `resample` functionality to enable resampling from different text units
*   `index` (?) for pre-tokenizing and indexing a corpus
*    specific textmodel types:
    *   Improve wordfish and round out methods
    *   Add CA scaling


## For bug reports and feature requests

Please use the issue page on the GitHub repository, or contact kbenoit@lse.ac.uk directly.



